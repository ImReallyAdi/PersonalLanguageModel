<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Pages API Client</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            min-height: 100vh;
        }
        .container {
            background: rgba(255, 255, 255, 0.1);
            padding: 30px;
            border-radius: 15px;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        }
        .status {
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
            font-weight: bold;
        }
        .loading { background: rgba(255, 193, 7, 0.3); }
        .ready { background: rgba(40, 167, 69, 0.3); }
        .error { background: rgba(220, 53, 69, 0.3); }
        .chat-container {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            min-height: 300px;
        }
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 8px;
        }
        .user-message {
            background: rgba(0, 123, 255, 0.3);
            text-align: right;
        }
        .ai-message {
            background: rgba(40, 167, 69, 0.3);
        }
        input[type="text"] {
            width: 70%;
            padding: 10px;
            border: none;
            border-radius: 5px;
            background: rgba(255, 255, 255, 0.9);
            color: #333;
        }
        button {
            padding: 10px 20px;
            background: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            margin-left: 10px;
        }
        button:hover {
            background: #0056b3;
        }
        button:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }
        .model-info {
            background: rgba(255, 255, 255, 0.05);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ Personal Language Model</h1>
        <p>Client-side AI running in your browser via WebAssembly</p>
        
        <div id="status" class="status loading">
            üîÑ Loading Python environment...
        </div>
        
        <div class="model-info" id="model-info" style="display: none;">
            <h3>Model Information</h3>
            <div id="model-details"></div>
        </div>
        
        <div class="chat-container" id="chat-container" style="display: none;">
            <div id="chat-messages"></div>
            <div style="margin-top: 20px;">
                <input type="text" id="message-input" placeholder="Type your message..." disabled>
                <button id="send-button" onclick="sendMessage()" disabled>Send</button>
            </div>
        </div>
        
        <div style="margin-top: 20px;">
            <button id="train-button" onclick="trainModel()" disabled>Train New Model</button>
            <button id="info-button" onclick="getModelInfo()" disabled>Get Model Info</button>
        </div>
    </div>

    <script>
        let pyodide;
        let modelLoaded = false;
        
        async function initializePyodide() {
            try {
                pyodide = await loadPyodide();
                
                // Install required packages
                await pyodide.loadPackage(['micropip']);
                await pyodide.runPython(`
                    import micropip
                    await micropip.install(['torch', 'numpy'])
                `);
                
                // Load our model code
                await pyodide.runPython(`
                    import torch
                    import torch.nn as nn
                    import numpy as np
                    from datetime import datetime
                    import json
                    import random
                    
                    # Global variables
                    current_model = None
                    char_to_idx = None
                    idx_to_char = None
                    
                    class SimpleTransformer(nn.Module):
                        def __init__(self, vocab_size, embed_dim=32, num_heads=2, num_layers=1, sequence_length=15):
                            super().__init__()
                            self.embed_dim = embed_dim
                            self.sequence_length = sequence_length
                            
                            self.embedding = nn.Embedding(vocab_size, embed_dim)
                            self.pos_encoding = nn.Parameter(torch.randn(sequence_length, embed_dim))
                            
                            encoder_layer = nn.TransformerEncoderLayer(
                                d_model=embed_dim,
                                nhead=num_heads,
                                dim_feedforward=embed_dim * 2,
                                dropout=0.0,
                                batch_first=True
                            )
                            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
                            self.output_layer = nn.Linear(embed_dim, vocab_size)
                        
                        def forward(self, src):
                            seq_len = src.size(1)
                            embeddings = self.embedding(src)
                            embeddings += self.pos_encoding[:seq_len].unsqueeze(0)
                            
                            mask = torch.triu(torch.ones(seq_len, seq_len)) == 1
                            mask = mask.transpose(0, 1)
                            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
                            
                            output = self.transformer(embeddings, mask=mask)
                            output = self.output_layer(output)
                            return output
                    
                    def train_demo_model():
                        global current_model, char_to_idx, idx_to_char
                        
                        demo_text = "Hello I am an AI assistant. I can help with questions and conversations. I enjoy talking about science technology history arts and life. I aim to provide helpful responses. What would you like to talk about? I can help with explanations writing problem solving and conversation. Thank you for chatting! How can I help you today? I hope you are having a great day. Feel free to ask me anything. I love learning and sharing knowledge."
                        
                        # Build vocabulary
                        chars = sorted(list(set(demo_text)))
                        char_to_idx = {ch: i for i, ch in enumerate(chars)}
                        idx_to_char = {i: ch for i, ch in enumerate(chars)}
                        
                        # Encode text
                        encoded_text = [char_to_idx[ch] for ch in demo_text]
                        
                        # Create model
                        model = SimpleTransformer(
                            vocab_size=len(char_to_idx),
                            embed_dim=32,
                            num_heads=2,
                            num_layers=1,
                            sequence_length=15
                        )
                        
                        # Train for a few epochs
                        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
                        model.train()
                        
                        sequence_length = 15
                        for epoch in range(3):
                            total_loss = 0
                            for i in range(0, len(encoded_text) - sequence_length, sequence_length):
                                if i + sequence_length + 1 >= len(encoded_text):
                                    break
                                
                                inputs = torch.tensor([encoded_text[i:i+sequence_length]], dtype=torch.long)
                                targets = torch.tensor([encoded_text[i+1:i+sequence_length+1]], dtype=torch.long)
                                
                                optimizer.zero_grad()
                                outputs = model(inputs)
                                loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.size(-1)), targets.view(-1))
                                loss.backward()
                                optimizer.step()
                                total_loss += loss.item()
                        
                        current_model = model
                        return True
                    
                    def generate_text(prompt="", max_length=50, temperature=0.8):
                        global current_model, char_to_idx, idx_to_char
                        
                        if current_model is None:
                            return "No model loaded"
                        
                        current_model.eval()
                        with torch.no_grad():
                            # Encode prompt
                            if prompt:
                                context = [char_to_idx.get(ch, 0) for ch in prompt]
                            else:
                                context = [0]
                            
                            # Generate
                            for _ in range(max_length):
                                if len(context) > current_model.sequence_length:
                                    context = context[-current_model.sequence_length:]
                                
                                input_tensor = torch.tensor([context], dtype=torch.long)
                                outputs = current_model(input_tensor)
                                logits = outputs[0, -1, :] / temperature
                                
                                # Sample from distribution
                                probs = torch.softmax(logits, dim=-1)
                                next_token = torch.multinomial(probs, 1).item()
                                context.append(next_token)
                            
                            # Decode
                            generated_text = ''.join([idx_to_char.get(idx, '') for idx in context])
                            return generated_text
                    
                    def get_model_info():
                        global current_model, char_to_idx
                        
                        if current_model is None:
                            return {
                                "status": "No model loaded",
                                "vocab_size": 0,
                                "parameters": 0
                            }
                        
                        total_params = sum(p.numel() for p in current_model.parameters())
                        
                        return {
                            "status": "Model loaded",
                            "vocab_size": len(char_to_idx) if char_to_idx else 0,
                            "parameters": total_params,
                            "embed_dim": current_model.embed_dim,
                            "sequence_length": current_model.sequence_length,
                            "timestamp": str(datetime.now())
                        }
                `);
                
                updateStatus("‚úÖ Python environment ready!", "ready");
                enableControls();
                
            } catch (error) {
                updateStatus("‚ùå Failed to load Python environment: " + error.message, "error");
                console.error("Pyodide loading error:", error);
            }
        }
        
        function updateStatus(message, type) {
            const status = document.getElementById('status');
            status.textContent = message;
            status.className = `status ${type}`;
        }
        
        function enableControls() {
            document.getElementById('train-button').disabled = false;
            document.getElementById('info-button').disabled = false;
            document.getElementById('chat-container').style.display = 'block';
        }
        
        async function trainModel() {
            updateStatus("üîÑ Training model...", "loading");
            document.getElementById('train-button').disabled = true;
            
            try {
                const result = await pyodide.runPython(`train_demo_model()`);
                if (result) {
                    updateStatus("‚úÖ Model trained successfully!", "ready");
                    modelLoaded = true;
                    document.getElementById('message-input').disabled = false;
                    document.getElementById('send-button').disabled = false;
                    await getModelInfo();
                } else {
                    updateStatus("‚ùå Model training failed", "error");
                }
            } catch (error) {
                updateStatus("‚ùå Training error: " + error.message, "error");
                console.error("Training error:", error);
            }
            
            document.getElementById('train-button').disabled = false;
        }
        
        async function getModelInfo() {
            try {
                const info = await pyodide.runPython(`
                    import json
                    json.dumps(get_model_info())
                `);
                const modelInfo = JSON.parse(info);
                
                document.getElementById('model-info').style.display = 'block';
                document.getElementById('model-details').innerHTML = \`
                    <strong>Status:</strong> \${modelInfo.status}<br>
                    <strong>Vocabulary Size:</strong> \${modelInfo.vocab_size}<br>
                    <strong>Parameters:</strong> \${modelInfo.parameters.toLocaleString()}<br>
                    <strong>Embedding Dimension:</strong> \${modelInfo.embed_dim || 'N/A'}<br>
                    <strong>Sequence Length:</strong> \${modelInfo.sequence_length || 'N/A'}
                \`;
            } catch (error) {
                console.error("Error getting model info:", error);
            }
        }
        
        async function sendMessage() {
            const input = document.getElementById('message-input');
            const message = input.value.trim();
            
            if (!message || !modelLoaded) return;
            
            // Add user message to chat
            addMessage(message, 'user');
            input.value = '';
            
            // Disable input while generating
            input.disabled = true;
            document.getElementById('send-button').disabled = true;
            
            try {
                const prompt = \`User: \${message}\\nAI:\`;
                const response = await pyodide.runPython(\`
                    generate_text("\${prompt.replace(/"/g, '\\\\"')}", max_length=40, temperature=0.8)
                \`);
                
                // Extract AI response
                let aiResponse = response;
                if (aiResponse.includes('AI:')) {
                    aiResponse = aiResponse.split('AI:').pop().trim();
                }
                
                // Clean up response
                aiResponse = aiResponse.replace(/User:.*$/g, '').trim();
                if (aiResponse.length > 100) {
                    aiResponse = aiResponse.substring(0, 100) + '...';
                }
                
                addMessage(aiResponse || "I'm still learning. Try training the model first!", 'ai');
                
            } catch (error) {
                addMessage("Sorry, I encountered an error: " + error.message, 'ai');
                console.error("Generation error:", error);
            }
            
            // Re-enable input
            input.disabled = false;
            document.getElementById('send-button').disabled = false;
            input.focus();
        }
        
        function addMessage(text, sender) {
            const chatMessages = document.getElementById('chat-messages');
            const messageDiv = document.createElement('div');
            messageDiv.className = \`message \${sender}-message\`;
            messageDiv.textContent = text;
            chatMessages.appendChild(messageDiv);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }
        
        // Handle Enter key in input
        document.addEventListener('DOMContentLoaded', function() {
            document.getElementById('message-input').addEventListener('keypress', function(e) {
                if (e.key === 'Enter') {
                    sendMessage();
                }
            });
        });
        
        // Initialize when page loads
        initializePyodide();
    </script>
</body>
</html>